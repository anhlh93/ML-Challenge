{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7MFCitF-IOn-",
   "metadata": {
    "id": "7MFCitF-IOn-"
   },
   "source": [
    "### Comments:\n",
    "*   DONE: Check year in publisher names, then update published years\n",
    "*   DONE: Add title_length column\n",
    "*   Compare performance with/without normalization\n",
    "*   Feature extraction\n",
    "*   Try XGBoost, SVM, CNN\n",
    "*   Other models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Be9yrMTGKY4n",
   "metadata": {
    "id": "Be9yrMTGKY4n"
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6ac630f",
   "metadata": {
    "id": "e6ac630f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\06.DOWNLOAD\\anaconda3_2\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json, re, ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from scipy.sparse import hstack, vstack\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.activations import linear, relu, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "G0-tKRd6K-8r",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G0-tKRd6K-8r",
    "outputId": "395ae572-2ee7-4250-8206-e4591afd9641"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\anhlh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anhlh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')  # Download the Punkt tokenizer\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421ac5a0",
   "metadata": {
    "id": "421ac5a0"
   },
   "source": [
    "## Load train data as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36c537bb",
   "metadata": {
    "id": "36c537bb"
   },
   "outputs": [],
   "source": [
    "def to_dataframe(json_path, csv_name):\n",
    "    # load json -> to csv -> to df\n",
    "    df = pd.read_json(json_path)\n",
    "    df.to_csv(csv_name, index=False)\n",
    "    result = pd.read_csv(csv_name)\n",
    "    # make all strings lowercase\n",
    "    result= result.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "    result['author'] = result['author'].fillna(value=\"[]\")\n",
    "    result['abstract'] = result['abstract'].fillna(value=\" \")\n",
    "    train = result.fillna(0)\n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5GL8ZMmUIRQ",
   "metadata": {
    "id": "e5GL8ZMmUIRQ"
   },
   "source": [
    "## Change author to number of authors, then normalize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "UowN-XEoUIlB",
   "metadata": {
    "id": "UowN-XEoUIlB"
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "def get_nr_authors(train):\n",
    "  train['author'] = train['author'].apply(ast.literal_eval)\n",
    "  train['author'] = train['author'].apply(len)\n",
    "\n",
    "  nr_authors_2d = train['author'].values.reshape(-1, 1)\n",
    "  train['author'] = scaler.fit_transform(nr_authors_2d)\n",
    "  return train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "S00f4u3a5nXS",
   "metadata": {
    "id": "S00f4u3a5nXS"
   },
   "source": [
    " COMMENT: adding a column counting the length of the title of the publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6BwQu9Ec5m2e",
   "metadata": {
    "id": "6BwQu9Ec5m2e"
   },
   "outputs": [],
   "source": [
    "def get_title_length(train):\n",
    "  train['title_length'] = train['title'].apply(lambda x: len(x.split()))\n",
    "  return train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620ecca3",
   "metadata": {
    "id": "620ecca3"
   },
   "source": [
    "## Encode entrytype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14ba54cc",
   "metadata": {
    "id": "14ba54cc"
   },
   "outputs": [],
   "source": [
    "def encode_entrytype(train):\n",
    "  # Get one ho t encoding of ENTRYTYPE column\n",
    "  one_hot = pd.get_dummies(train['ENTRYTYPE'])\n",
    "  one_hot = one_hot.astype(int)\n",
    "  train = train.join(one_hot)\n",
    "  train = train.drop('ENTRYTYPE', axis=1)\n",
    "  return train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6119db2c",
   "metadata": {
    "id": "6119db2c"
   },
   "source": [
    "## Get the first and last years in which a publisher appeared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5qlLZmVLOmXY",
   "metadata": {
    "id": "5qlLZmVLOmXY"
   },
   "source": [
    "###### 1. Modify the inconsistent publisher names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2daee4fc",
   "metadata": {
    "id": "2daee4fc"
   },
   "outputs": [],
   "source": [
    "inconsistency = ['atala', 'aslib', 'european language resources association', 'incoma', 'springer',\n",
    "                'international committee on computational linguistics', 'link√∂ping university electronic press',\n",
    "                'northern european association for language technology', 'the korean society for language and information']\n",
    "\n",
    "\n",
    "def rename_publishers(df, inconsistency):\n",
    "    df.publisher = np.where((df.publisher.str.contains(\"not mentionned\")),\n",
    "                            \"unknown\", df.publisher)\n",
    "    df.publisher = np.where((df.publisher.str.contains(\"association for computational linguistics\"))& (~df.publisher.str.contains(\"and\"))|\n",
    "                            (df.publisher == \"assocation for computational linguistics\")|\n",
    "                            (df.publisher == \"association for computational lingustics\"),\n",
    "                            \"association for computational linguistics\", df.publisher)\n",
    "    df.publisher = np.where((df.publisher.str.contains(\"northern european association\"))&\n",
    "                             (df.publisher.str.contains(\"language technology\")),\n",
    "                             \"northern european association for language technology\", df.publisher)\n",
    "    df.publisher = np.where((df.publisher.str.contains(\"international committee\"))&\n",
    "                             (df.publisher.str.contains(\"computational linguistics\")),\n",
    "                             \"international committee for computational linguistics\", df.publisher)\n",
    "    df.publisher = np.where((df.publisher.str.contains(\"european language\")),\n",
    "                             \"european language resources association\", df.publisher)\n",
    "\n",
    "    for incon in inconsistency:\n",
    "        df.publisher = np.where(df.publisher.str.contains(incon), incon, df.publisher)\n",
    "\n",
    "    df = df[df[\"publisher\"] != \"unknown\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9j-d6ywoOur8",
   "metadata": {
    "id": "9j-d6ywoOur8"
   },
   "source": [
    "###### 2. Get the first and last years in which a publisher appeared\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "t2u7opM4hf32",
   "metadata": {
    "id": "t2u7opM4hf32"
   },
   "outputs": [],
   "source": [
    "years_in_names = list(range(1962, 2024))\n",
    "\n",
    "def manual_update_years(pub_agg, years_in_names):\n",
    "  \"\"\"Updates the first and last years if the name of the publisher contains a year\"\"\"\n",
    "  for year in years_in_names:\n",
    "    pub_agg['first_published'] = np.where(\n",
    "                                          (pub_agg.publisher.str.contains(str(year))),\n",
    "                                          year,\n",
    "                                          pub_agg['first_published']\n",
    "                                        )\n",
    "    pub_agg['last_published'] = np.where(\n",
    "                                          (pub_agg.publisher.str.contains(str(year))),\n",
    "                                          year,\n",
    "                                          pub_agg['last_published']\n",
    "                                        )\n",
    "  return pub_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "NIPs3tTaOHKj",
   "metadata": {
    "id": "NIPs3tTaOHKj"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "def publisher_years(train, inconsistency):\n",
    "  train = rename_publishers(train, inconsistency)\n",
    "  pub = train[[\"publisher\", \"year\"]]\n",
    "  pub_agg = pub.groupby(\"publisher\").agg(first_published=('year','min'),\n",
    "                                        last_published=('year','max')).reset_index()\n",
    "  pub_agg = manual_update_years(pub_agg, years_in_names)\n",
    "  pub_agg[['first_published', 'last_published']] = scaler.fit_transform(pub_agg[['first_published', 'last_published']])\n",
    "  return pub_agg\n",
    "\n",
    "\n",
    "def join_published_years(train, pub_agg):\n",
    "  #Join the pub_agg with train\n",
    "  train_processed = pd.merge(train, pub_agg, on='publisher', how='right')\n",
    "  train_processed = train_processed.drop('publisher', axis=1)\n",
    "  return train_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "042db4a3",
   "metadata": {
    "id": "042db4a3"
   },
   "outputs": [],
   "source": [
    "##check the specfic publishers if you want:\n",
    "# pub_agg.publisher.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ca3cb48",
   "metadata": {
    "id": "3ca3cb48"
   },
   "outputs": [],
   "source": [
    "##check the publishers that are long gone:\n",
    "# pub_agg.sort_values(by=[\"last_published\"], ascending=True).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Cg-TXN_xLL6F",
   "metadata": {
    "id": "Cg-TXN_xLL6F"
   },
   "source": [
    "## Extract information from 'title' column\n",
    "### 1.1 Tokenize the title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "F5wAKBrQLMeB",
   "metadata": {
    "id": "F5wAKBrQLMeB"
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "#Tokenization function\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    new_tokens = []\n",
    "    for word in tokens:\n",
    "        #Remove special characters and stop words in the tokens\n",
    "        word = re.sub(r'[^A-Za-z\\s]', '', word)\n",
    "        if word not in stop_words and word != '':\n",
    "            # Reduce words to their root form to handle variations\n",
    "            word = stemmer.stem(word)\n",
    "            new_tokens.append(word)\n",
    "    tokenized_title = ' '.join(new_tokens)\n",
    "    return tokenized_title\n",
    "\n",
    "\n",
    "#Apply tokenization to the 'title' column\n",
    "#Notice that the original title column is overwritten.\n",
    "###For saving time you may check the first 100 rows before applying to the whloe dataframe:\n",
    "#train = train.loc[0:100]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JEWkvSp48l36",
   "metadata": {
    "id": "JEWkvSp48l36"
   },
   "source": [
    "## 1.2 Tokenize the abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kLwqsenRUg5H",
   "metadata": {
    "id": "kLwqsenRUg5H"
   },
   "source": [
    "### 2. Stemming by Vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bYhZ1f6UB6Wt",
   "metadata": {
    "id": "bYhZ1f6UB6Wt"
   },
   "outputs": [],
   "source": [
    "# Generator function to yield batches of text\n",
    "def batch_generator(data, batch_size):\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i:i + batch_size]\n",
    "\n",
    "# HashingVectorizer with incremental learning\n",
    "vectorizer = HashingVectorizer(n_features=1000, alternate_sign=False)\n",
    "\n",
    "# Process data in batches\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aT9Ry694819d",
   "metadata": {
    "id": "aT9Ry694819d"
   },
   "outputs": [],
   "source": [
    "def stem_title(train):\n",
    "  sparse_data_list = []\n",
    "  for batch in batch_generator(train['title'], batch_size):\n",
    "      X_batch = vectorizer.transform(batch)\n",
    "      sparse_data_list.append(X_batch)\n",
    "\n",
    "  # Concatenate the sparse matrices vertically\n",
    "  X_sparse = vstack(sparse_data_list, format='csr')\n",
    "\n",
    "  # Convert the sparse matrix to a DataFrame\n",
    "  vectorized_df = pd.DataFrame(X_sparse.toarray(), columns=[f'feature_{i}' for i in range(X_sparse.shape[1])])\n",
    "\n",
    "  # Concatenate the original DataFrame and the vectorized DataFrame horizontally\n",
    "  train = pd.concat([train, vectorized_df], axis=1)\n",
    "  train = train.drop('title', axis=1)\n",
    "  return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "P_GdLEkY87pz",
   "metadata": {
    "id": "P_GdLEkY87pz"
   },
   "outputs": [],
   "source": [
    "def stem_abstract(train):\n",
    "  sparse_data_list_abs = []\n",
    "  for batch in batch_generator(train['abstract'], batch_size):\n",
    "      abs_batch = vectorizer.transform(batch)\n",
    "      sparse_data_list_abs.append(abs_batch)\n",
    "\n",
    "  # Concatenate the sparse matrices vertically\n",
    "  abs_sparse = vstack(sparse_data_list_abs, format='csr')\n",
    "\n",
    "  # Convert the sparse matrix to a DataFrame\n",
    "  vectorized_df = pd.DataFrame(abs_sparse.toarray(), columns=[f'feature_{i}' for i in range(abs_sparse.shape[1])])\n",
    "\n",
    "  # Concatenate the original DataFrame and the vectorized DataFrame horizontally\n",
    "  train = pd.concat([train, vectorized_df], axis=1)\n",
    "  train = train.drop('abstract', axis=1)\n",
    "  return train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Acav0HmgS513",
   "metadata": {
    "id": "Acav0HmgS513"
   },
   "source": [
    "# Load train and test, then pre-process them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2225084",
   "metadata": {
    "id": "d2225084"
   },
   "outputs": [],
   "source": [
    "def pre_process(train, pub_agg):\n",
    "  train = train.drop(['editor'], axis=1)\n",
    "  train = get_nr_authors(train)\n",
    "  train = get_title_length(train)\n",
    "  train = encode_entrytype(train)\n",
    "  train = rename_publishers(train, inconsistency)\n",
    "  train = join_published_years(train, pub_agg)\n",
    "  train['title'] = train['title'].apply(tokenize_text)\n",
    "  train['title'] = train['title'].fillna(value=\"\")\n",
    "  train['abstract'] = train['abstract'].apply(tokenize_text)\n",
    "  train['abstract'] = train['abstract'].fillna(value=\"\")\n",
    "  train = stem_title(train)\n",
    "  train = stem_abstract(train)\n",
    "  return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "zEvJfNoOVE51",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zEvJfNoOVE51",
    "outputId": "a7ef9260-2d1d-4ede-b1fd-f53d89a162e7"
   },
   "outputs": [],
   "source": [
    "train = to_dataframe('train.json', 'train.csv')\n",
    "test = to_dataframe('test.json', 'test.csv')\n",
    "pub_agg = publisher_years(train, inconsistency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "VSQhdnbHPzgd",
   "metadata": {
    "id": "VSQhdnbHPzgd"
   },
   "outputs": [],
   "source": [
    "train = pre_process(train, pub_agg)\n",
    "# print(train.dtypes)\n",
    "test = pre_process(test, pub_agg)\n",
    "# print(test.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "244mkTjshHTY",
   "metadata": {
    "id": "244mkTjshHTY"
   },
   "outputs": [],
   "source": [
    "X = train.drop('year', axis=1)\n",
    "y = train['year'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ZmVSkdZPaucM",
   "metadata": {
    "id": "ZmVSkdZPaucM"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a_736Enkjg0b",
   "metadata": {
    "id": "a_736Enkjg0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     author  title_length  article  inproceedings  proceedings  \\\n",
      "0  0.351939            12        0              1            0   \n",
      "1  0.351939            12        0              1            0   \n",
      "2  0.351939            15        0              1            0   \n",
      "3  2.965848            20        0              1            0   \n",
      "4 -1.390666            20        0              0            1   \n",
      "\n",
      "   first_published  last_published  feature_0  feature_1  feature_2  ...  \\\n",
      "0         1.125159        1.038567        0.0        0.0        0.0  ...   \n",
      "1         1.125159        1.038567        0.0        0.0        0.0  ...   \n",
      "2         1.125159        1.038567        0.0        0.0        0.0  ...   \n",
      "3         1.125159        1.038567        0.0        0.0        0.0  ...   \n",
      "4         1.125159        1.038567        0.0        0.0        0.0  ...   \n",
      "\n",
      "   feature_990  feature_991  feature_992  feature_993  feature_994  \\\n",
      "0          0.0          0.0          0.0          0.0          0.0   \n",
      "1          0.0          0.0          0.0          0.0          0.0   \n",
      "2          0.0          0.0          0.0          0.0          0.0   \n",
      "3          0.0          0.0          0.0          0.0          0.0   \n",
      "4          0.0          0.0          0.0          0.0          0.0   \n",
      "\n",
      "   feature_995  feature_996  feature_997  feature_998  feature_999  \n",
      "0          0.0          0.0          0.0          0.0          0.0  \n",
      "1          0.0          0.0          0.0          0.0          0.0  \n",
      "2          0.0          0.0          0.0          0.0          0.0  \n",
      "3          0.0          0.0          0.0          0.0          0.0  \n",
      "4          0.0          0.0          0.0          0.0          0.0  \n",
      "\n",
      "[5 rows x 2007 columns]\n"
     ]
    }
   ],
   "source": [
    "print(test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "K3Tm8xJEh-8h",
   "metadata": {
    "id": "K3Tm8xJEh-8h"
   },
   "source": [
    "# Regression / Multi-class Classification in Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dhLL3Ro4hn5n",
   "metadata": {
    "id": "dhLL3Ro4hn5n"
   },
   "source": [
    "Implementation for Softmax Regression:\n",
    "1. Basic\n",
    "2. Use a linear activation function in the output layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ckL1E1nzVliW",
   "metadata": {
    "id": "ckL1E1nzVliW"
   },
   "source": [
    "## MLPRegressor\n",
    "### MAE: 3.952511946715513"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jywGPEf-PGVA",
   "metadata": {
    "id": "jywGPEf-PGVA"
   },
   "source": [
    "**The model below takes approximately 15 min to run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ywnIdhXGmZI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "2ywnIdhXGmZI",
    "outputId": "5cac4f21-7850-49c2-e19c-00b80acd1633"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=11)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPRegressor</label><div class=\"sk-toggleable__content\"><pre>MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=11)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=11)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPRegressor(hidden_layer_sizes=(100, 50),\n",
    "                   max_iter=1000,\n",
    "                   random_state=11,\n",
    "                   activation='relu',   # Use ReLU activation in hidden layers\n",
    "                   solver='adam',       # Use the Adam optimizer\n",
    "                   alpha=0.0001,        # L2 regularization term\n",
    "                   )\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZctlBSJjbUQK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZctlBSJjbUQK",
    "outputId": "823546f5-c91a-43de-b38d-20fb7f796751"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.952511946715513"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on the validation set\n",
    "y_val_pred_mlp = mlp.predict(X_val)\n",
    "# Evaluate the model on the validation set\n",
    "mae_val = mean_absolute_error(y_val, y_val_pred_mlp)\n",
    "mae_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G9BvdMSBVdK9",
   "metadata": {
    "id": "G9BvdMSBVdK9"
   },
   "source": [
    "## MLPClassfier\n",
    "### MAE: 3.5305892547660314"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7-AIZYSKPNM9",
   "metadata": {
    "id": "7-AIZYSKPNM9"
   },
   "source": [
    "**The model below took around 10min**\n",
    "But it took 42min using uni wifi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z8fsfMRIXBPk",
   "metadata": {
    "id": "z8fsfMRIXBPk"
   },
   "outputs": [],
   "source": [
    "mlp_classifier = MLPClassifier(\n",
    "                                hidden_layer_sizes=(50, 25),\n",
    "                                max_iter=500,\n",
    "                                random_state=11,\n",
    "                                activation='logistic' # 'logistic' activates the softmax function\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "njveP9TPg5D8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 148
    },
    "id": "njveP9TPg5D8",
    "outputId": "e1bea242-c002-4dba-8b97-de0d99f89ec9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, hidden_layer_sizes=(50, 25), max_iter=500,\n",
       "              random_state=11)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, hidden_layer_sizes=(50, 25), max_iter=500,\n",
       "              random_state=11)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(activation='logistic', hidden_layer_sizes=(50, 25), max_iter=500,\n",
       "              random_state=11)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Y3LB0jyfTDFO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y3LB0jyfTDFO",
    "outputId": "787fedcd-5241-481a-b318-c2808b8f7280"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5305892547660314\n",
      "[2020 2016 2023 2022 2021 2011 2023 2021 2022 2012]\n",
      "[2021 2008 2017 2021 2017 2008 2019 2020 2020 2001]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the validation set\n",
    "y_val_pred_mlp_c = mlp_classifier.predict(X_val)\n",
    "# Calculate MAE on the validation set\n",
    "mae_val = mean_absolute_error(y_val, y_val_pred_mlp_c)\n",
    "print(mae_val) #3.5305892547660314\n",
    "print(y_val[0:10])\n",
    "print(y_val_pred_mlp_c[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-j0kgW6niiRA",
   "metadata": {
    "id": "-j0kgW6niiRA"
   },
   "source": [
    "# Regression / Multi-class Classification in Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2Z-tlouXWPv9",
   "metadata": {
    "id": "2Z-tlouXWPv9"
   },
   "source": [
    "##DecisionTree Regressor\n",
    "###MAE: 3.4001201668941414\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JKc99jQvgkdi",
   "metadata": {
    "id": "JKc99jQvgkdi"
   },
   "source": [
    "took 27min to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZuJ0-87DWQ7C",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "id": "ZuJ0-87DWQ7C",
    "outputId": "779d0439-2747-4b6d-f2c0-380530290313"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-522b3848764c>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Use GridSearchCV to search through the parameter grid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'neg_mean_absolute_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Get the best hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1865\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1792\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1793\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \"\"\"\n\u001b[1;32m   1246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m   1248\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    377\u001b[0m             )\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [None, 5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "dtr = DecisionTreeRegressor(random_state=11)\n",
    "\n",
    "# Use GridSearchCV to search through the parameter grid\n",
    "grid_search = GridSearchCV(dtr, param_grid, cv=5, scoring='neg_mean_absolute_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "\n",
    "# Evaluate the model with the best hyperparameters\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_val)\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "print(f\"Mean Absolute Error on Validation Set: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29lTH3udVz_n",
   "metadata": {
    "id": "29lTH3udVz_n"
   },
   "source": [
    "## DecisionTreeClassifier\n",
    "### MAE: 3.787175043327556\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6S76NctSXBWh",
   "metadata": {
    "id": "6S76NctSXBWh"
   },
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(\n",
    "                              random_state=11,\n",
    "\n",
    "                            )      # Make an instance of the Model\n",
    "dtc.fit(X_train, y_train)           # Training the model on the data (train set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ZbfyPddXlCA",
   "metadata": {
    "id": "4ZbfyPddXlCA"
   },
   "outputs": [],
   "source": [
    "# Make predictions on the validation set\n",
    "y_val_pred_dtc = dtc.predict(X_val)\n",
    "# Evaluate the model on the validation set\n",
    "mae_val = mean_absolute_error(y_val, y_val_pred_dtc)\n",
    "mae_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0_w-cjctV3Df",
   "metadata": {
    "id": "0_w-cjctV3Df"
   },
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "cross_val_scores = cross_val_score(dtc, X, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "\n",
    "# Display the cross-validation scores\n",
    "print(\"Cross-Validation Scores:\", -cross_val_scores)  # Negate scores to get positive MAE values\n",
    "print(\"Mean MAE:\", -cross_val_scores.mean())           # Calculate mean MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Cq4Njnxorbr6",
   "metadata": {
    "id": "Cq4Njnxorbr6"
   },
   "source": [
    "# Regression / Multi-class Classification in Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zq6qHKCeVuKL",
   "metadata": {
    "id": "zq6qHKCeVuKL"
   },
   "source": [
    "## RandomForestRegressor\n",
    "### MAE: 2.987849547676082"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uWmGl1v3PaSn",
   "metadata": {
    "id": "uWmGl1v3PaSn"
   },
   "source": [
    "**The model below takes approximately 57 min to run!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CP3NMcM3uhqh",
   "metadata": {
    "id": "CP3NMcM3uhqh"
   },
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor(random_state=11)      # Make an instance of the Model\n",
    "rfr.fit(X_train, y_train)           # Training the model on the data (train set)\n",
    "\n",
    "y_val_pred_rfr = rfr.predict(X_val)\n",
    "mae_score = mean_absolute_error(y_val, y_val_pred_rfr)\n",
    "print('MAE on validation set: ', mae_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "M9hL4QcMVaNB",
   "metadata": {
    "id": "M9hL4QcMVaNB"
   },
   "source": [
    "##RandomForestRegressor with tuning\n",
    "###MAE: 2.987849547676082"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hygwDI1OK-CD",
   "metadata": {
    "id": "hygwDI1OK-CD"
   },
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    'n_estimators': 100,      # Number of trees in the forest\n",
    "    'max_depth': None,        # Maximum depth of the tree\n",
    "    'min_samples_split': 2,   # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': 1,    # Minimum number of samples required to be at a leaf node\n",
    "    'random_state':11\n",
    "}\n",
    "\n",
    "# Create an instance of the model with specified hyperparameters\n",
    "rf = RandomForestRegressor(**rf_params)\n",
    "rf.fit(X_train, y_train)           # Training the model on the data (train set)\n",
    "\n",
    "y_val_pred_rf = rf.predict(X_val)\n",
    "mae_score = mean_absolute_error(y_val, y_val_pred_rf)\n",
    "print('MAE on validation set: ', mae_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "D7j0Dt0oe9L9",
   "metadata": {
    "id": "D7j0Dt0oe9L9"
   },
   "source": [
    "##RandomForestRegressor with tuning 2.0\n",
    "###MAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6sJjR_SKfAek",
   "metadata": {
    "id": "6sJjR_SKfAek"
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [None, 5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "rf = RandomForestRegressor(random_state=11)\n",
    "\n",
    "# Use GridSearchCV to search through the parameter grid\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='neg_mean_absolute_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "\n",
    "# Evaluate the model with the best hyperparameters\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_val)\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "print(f\"Mean Absolute Error on Validation Set: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iO_A60MZJ1h9",
   "metadata": {
    "id": "iO_A60MZJ1h9"
   },
   "outputs": [],
   "source": [
    "print(y_val[0:50])\n",
    "print(y_val_pred_dtc[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe827be",
   "metadata": {
    "id": "afe827be"
   },
   "source": [
    "# Experiment Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "STDSYeKCJQ2o",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "STDSYeKCJQ2o",
    "outputId": "c7b3a197-9353-43f7-a560-746c3eb575af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1443/1443 [==============================] - 9s 6ms/step - loss: 2013.9005 - val_loss: 2012.6420\n",
      "Epoch 2/50\n",
      "1443/1443 [==============================] - 6s 4ms/step - loss: 2013.9014 - val_loss: 2012.6420\n",
      "Epoch 3/50\n",
      "1443/1443 [==============================] - 8s 6ms/step - loss: 2013.8992 - val_loss: 2012.6420\n",
      "Epoch 4/50\n",
      "1443/1443 [==============================] - 6s 4ms/step - loss: 2013.8993 - val_loss: 2012.6420\n",
      "Epoch 5/50\n",
      "1443/1443 [==============================] - 10s 7ms/step - loss: 2013.9021 - val_loss: 2012.6420\n",
      "Epoch 6/50\n",
      "1443/1443 [==============================] - 8s 5ms/step - loss: 2013.8998 - val_loss: 2012.6420\n",
      "Epoch 7/50\n",
      "1443/1443 [==============================] - 7s 5ms/step - loss: 2013.9014 - val_loss: 2012.6420\n",
      "Epoch 8/50\n",
      "1443/1443 [==============================] - 8s 5ms/step - loss: 2013.9000 - val_loss: 2012.6420\n",
      "Epoch 9/50\n",
      "1443/1443 [==============================] - 7s 5ms/step - loss: 2013.9006 - val_loss: 2012.6420\n",
      "Epoch 10/50\n",
      "1443/1443 [==============================] - 8s 5ms/step - loss: 2013.9016 - val_loss: 2012.6420\n",
      "Epoch 11/50\n",
      "1443/1443 [==============================] - 7s 5ms/step - loss: 2013.9016 - val_loss: 2012.6420\n",
      "Epoch 12/50\n",
      "1443/1443 [==============================] - 8s 6ms/step - loss: 2013.8989 - val_loss: 2012.6420\n",
      "Epoch 13/50\n",
      "1443/1443 [==============================] - 7s 5ms/step - loss: 2013.9017 - val_loss: 2012.6420\n",
      "Epoch 14/50\n",
      "1443/1443 [==============================] - 8s 6ms/step - loss: 2013.9004 - val_loss: 2012.6420\n",
      "Epoch 15/50\n",
      "1443/1443 [==============================] - 6s 4ms/step - loss: 2013.8988 - val_loss: 2012.6420\n",
      "Epoch 16/50\n",
      "1443/1443 [==============================] - 8s 6ms/step - loss: 2013.9021 - val_loss: 2012.6420\n",
      "Epoch 17/50\n",
      "1443/1443 [==============================] - 6s 4ms/step - loss: 2013.9017 - val_loss: 2012.6420\n",
      "Epoch 18/50\n",
      "1443/1443 [==============================] - 8s 6ms/step - loss: 2013.8988 - val_loss: 2012.6420\n",
      "Epoch 19/50\n",
      "1443/1443 [==============================] - 8s 5ms/step - loss: 2013.9001 - val_loss: 2012.6420\n",
      "Epoch 20/50\n",
      "1443/1443 [==============================] - 9s 6ms/step - loss: 2013.9019 - val_loss: 2012.6420\n",
      "Epoch 21/50\n",
      "1443/1443 [==============================] - 6s 4ms/step - loss: 2013.9031 - val_loss: 2012.6420\n",
      "Epoch 22/50\n",
      "1443/1443 [==============================] - 8s 6ms/step - loss: 2013.9009 - val_loss: 2012.6420\n",
      "Epoch 23/50\n",
      "1443/1443 [==============================] - 6s 4ms/step - loss: 2013.8979 - val_loss: 2012.6420\n",
      "Epoch 24/50\n",
      "1443/1443 [==============================] - 8s 6ms/step - loss: 2013.9017 - val_loss: 2012.6420\n",
      "Epoch 25/50\n",
      "1443/1443 [==============================] - 6s 4ms/step - loss: 2013.9009 - val_loss: 2012.6420\n",
      "Epoch 26/50\n",
      "1443/1443 [==============================] - 8s 6ms/step - loss: 2013.9001 - val_loss: 2012.6420\n",
      "Epoch 27/50\n",
      "1443/1443 [==============================] - 6s 4ms/step - loss: 2013.8998 - val_loss: 2012.6420\n",
      "Epoch 28/50\n",
      "1443/1443 [==============================] - 8s 5ms/step - loss: 2013.9006 - val_loss: 2012.6420\n",
      "Epoch 29/50\n",
      "1443/1443 [==============================] - 6s 4ms/step - loss: 2013.9012 - val_loss: 2012.6420\n",
      "Epoch 30/50\n",
      "1443/1443 [==============================] - 8s 5ms/step - loss: 2013.9009 - val_loss: 2012.6420\n",
      "Epoch 31/50\n",
      "1443/1443 [==============================] - 6s 4ms/step - loss: 2013.9010 - val_loss: 2012.6420\n",
      "Epoch 32/50\n",
      "1443/1443 [==============================] - 8s 6ms/step - loss: 2013.9006 - val_loss: 2012.6420\n",
      "Epoch 33/50\n",
      "1443/1443 [==============================] - 7s 5ms/step - loss: 2013.9005 - val_loss: 2012.6420\n",
      "Epoch 34/50\n",
      "1443/1443 [==============================] - 8s 6ms/step - loss: 2013.9016 - val_loss: 2012.6420\n",
      "Epoch 35/50\n",
      "1443/1443 [==============================] - 7s 5ms/step - loss: 2013.8994 - val_loss: 2012.6420\n",
      "Epoch 36/50\n",
      "1443/1443 [==============================] - 8s 6ms/step - loss: 2013.8992 - val_loss: 2012.6420\n",
      "Epoch 37/50\n",
      "1443/1443 [==============================] - 7s 5ms/step - loss: 2013.8972 - val_loss: 2012.6420\n",
      "Epoch 38/50\n",
      "1443/1443 [==============================] - 9s 6ms/step - loss: 2013.9010 - val_loss: 2012.6420\n",
      "Epoch 39/50\n",
      "1443/1443 [==============================] - 7s 5ms/step - loss: 2013.8989 - val_loss: 2012.6420\n",
      "Epoch 40/50\n",
      "1443/1443 [==============================] - 8s 6ms/step - loss: 2013.9006 - val_loss: 2012.6420\n",
      "Epoch 41/50\n",
      "1443/1443 [==============================] - 7s 5ms/step - loss: 2013.9000 - val_loss: 2012.6420\n",
      "Epoch 42/50\n",
      "1443/1443 [==============================] - 8s 5ms/step - loss: 2013.9042 - val_loss: 2012.6420\n",
      "Epoch 43/50\n",
      "1443/1443 [==============================] - 7s 5ms/step - loss: 2013.9012 - val_loss: 2012.6420\n",
      "Epoch 44/50\n",
      "1443/1443 [==============================] - 8s 5ms/step - loss: 2013.8998 - val_loss: 2012.6420\n",
      "Epoch 45/50\n",
      "1443/1443 [==============================] - 8s 5ms/step - loss: 2013.9014 - val_loss: 2012.6420\n",
      "Epoch 46/50\n",
      "1443/1443 [==============================] - 7s 5ms/step - loss: 2013.9006 - val_loss: 2012.6420\n",
      "Epoch 47/50\n",
      "1443/1443 [==============================] - 8s 6ms/step - loss: 2013.9021 - val_loss: 2012.6420\n",
      "Epoch 48/50\n",
      "1443/1443 [==============================] - 7s 5ms/step - loss: 2013.9009 - val_loss: 2012.6420\n",
      "Epoch 49/50\n",
      "1443/1443 [==============================] - 8s 5ms/step - loss: 2013.9012 - val_loss: 2012.6420\n",
      "Epoch 50/50\n",
      "1443/1443 [==============================] - 7s 5ms/step - loss: 2013.9012 - val_loss: 2012.6420\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7b50ef3e7fa0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(units=100, activation='relu'),\n",
    "    Dense(units=50, activation='relu'),\n",
    "    Dense(units=1, activation='softmax')  # Linear activation for regression\n",
    "], name=\"basic\")\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_clean.drop('year', axis=1), train_clean['year'].values, epochs=50, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Lmhy6THELWWq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lmhy6THELWWq",
    "outputId": "bfa8fd58-d067-4bfe-c6f6-cd6e6e6f0289"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1803/1803 [==============================] - 4s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "train_predictions = model.predict(train_clean.drop('year', axis=1))\n",
    "mae_train = tf.keras.losses.mean_absolute_error(train_clean['year'].values, train_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YRP5P1UDhmlu",
   "metadata": {
    "id": "YRP5P1UDhmlu"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234) # for consistent results\n",
    "model = Sequential(\n",
    "    [\n",
    "        ### START CODE HERE ###\n",
    "        Dense(units=128, activation='relu'),\n",
    "        Dense(units=64, activation='relu'),\n",
    "        Dense(units=50, activation='softmax')\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "    ], name = \"basic\"\n",
    ")\n",
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sS8npHNyXBJW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sS8npHNyXBJW",
    "outputId": "66f0998a-c10e-4678-a2b1-a99c4351d294"
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-673687393b27>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_clean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_clean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     ]\n\u001b[0;32m---> 60\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     61\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-19-673687393b27>\", line 1, in <cell line: 1>\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1783, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1377, in train_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1360, in step_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1349, in run_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1127, in train_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1185, in compute_loss\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 143, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 270, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 2454, in sparse_categorical_crossentropy\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5777, in sparse_categorical_crossentropy\n\nReceived a label value of 2023 which is outside the valid range of [0, 50).  Label values: 2012 2007 2000 2021 2002 2019 2012 2005 2018 2012 2020 2021 2013 2011 2023 2002 2020 2014 2021 2007 2007 2022 2013 2020 2022 2005 2022 2021 2015 2022 2019 2021\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_705]"
     ]
    }
   ],
   "source": [
    "model.fit(train_clean.drop('year', axis=1), train_clean['year'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af73991",
   "metadata": {
    "id": "7af73991"
   },
   "outputs": [],
   "source": [
    "len(train.editor.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bfa990",
   "metadata": {
    "id": "03bfa990"
   },
   "outputs": [],
   "source": [
    "# The paper with the most authors\n",
    "from ast import literal_eval\n",
    "maximum = 0\n",
    "total_authors = sorted_train.author.unique()\n",
    "\n",
    "for author in total_authors:\n",
    "    if type(author) is str:\n",
    "        author = literal_eval(author)\n",
    "        if len(author) > maximum:\n",
    "            maximum = len(author)\n",
    "            longest_name = author\n",
    "\n",
    "print(maximum, longest_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0393d85b",
   "metadata": {
    "id": "0393d85b"
   },
   "outputs": [],
   "source": [
    "dummy = make_pipeline(featurizer, DummyRegressor(strategy='mean'))\n",
    "ridge = make_pipeline(featurizer, Ridge())\n",
    "\n",
    "dummy.fit(train.drop('year', axis=1), train['year'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iFAoUca16LNS",
   "metadata": {
    "id": "iFAoUca16LNS"
   },
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import HashingVectorizer\n",
    "# import pandas as pd\n",
    "\n",
    "# # Vectorization using HashingVectorizer\n",
    "# vectorizer = HashingVectorizer()  # Adjust n_features as needed\n",
    "# X = vectorizer.transform(train_processed['title'])\n",
    "\n",
    "# # Convert the sparse matrix to a DataFrame\n",
    "# count_df = pd.DataFrame(X.toarray(), columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
    "\n",
    "# # Concatenate the HashingVectorizer DataFrame with the original DataFrame\n",
    "# # train_processed = pd.concat([train_processed, count_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6-tWME8uUiZ5",
   "metadata": {
    "id": "6-tWME8uUiZ5"
   },
   "outputs": [],
   "source": [
    "# # Vectorization\n",
    "# vectorizer = CountVectorizer()\n",
    "# X = vectorizer.fit_transform(train_processed['title'])\n",
    "# count_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# # Concatenate the TF-IDF DataFrame with the original DataFrame\n",
    "# train_processed= pd.concat([train_processed, count_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82K6rT2-8ZZu",
   "metadata": {
    "id": "82K6rT2-8ZZu"
   },
   "outputs": [],
   "source": [
    "# train_processed = pd.concat([train_processed, count_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5T4h7q3HUigI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5T4h7q3HUigI",
    "outputId": "0eabadd7-04ce-43e7-a464-06fe4aeb54a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57696, 27134)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurizer = ColumnTransformer(transformers=[(\"title\", CountVectorizer(), \"title\")])\n",
    "featurizer.fit(train_processed[['title']])\n",
    "\n",
    "transformed_title = featurizer.transform(train_processed[['title']])\n",
    "transformed_title.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DSszfeY64c6I",
   "metadata": {
    "id": "DSszfeY64c6I"
   },
   "outputs": [],
   "source": [
    "# df_transformed_title = pd.DataFrame(transformed_title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ICguyaroFcdW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ICguyaroFcdW",
    "outputId": "fa119597-9b94-4b1a-b531-192ed2689c84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year               int64\n",
       "author           float64\n",
       "abstract          object\n",
       "article            int64\n",
       "inproceedings      int64\n",
       "                  ...   \n",
       "feature_995      float64\n",
       "feature_996      float64\n",
       "feature_997      float64\n",
       "feature_998      float64\n",
       "feature_999      float64\n",
       "Length: 1008, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_clean.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "T7hoEXfnFeOG",
   "metadata": {
    "id": "T7hoEXfnFeOG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "957451c4",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7de81a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboostNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Obtaining dependency information for xgboost from https://files.pythonhosted.org/packages/bc/43/242432efc3f60052a4a534dc4926b21e236ab4ec8d4920c593da3f65c65d/xgboost-2.0.2-py3-none-win_amd64.whl.metadata\n",
      "  Downloading xgboost-2.0.2-py3-none-win_amd64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: numpy in d:\\06.download\\anaconda3_2\\lib\\site-packages (from xgboost) (1.24.3)\n",
      "Requirement already satisfied: scipy in d:\\06.download\\anaconda3_2\\lib\\site-packages (from xgboost) (1.11.1)\n",
      "Downloading xgboost-2.0.2-py3-none-win_amd64.whl (99.8 MB)\n",
      "   ---------------------------------------- 0.0/99.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/99.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/99.8 MB 653.6 kB/s eta 0:02:33\n",
      "   ---------------------------------------- 0.3/99.8 MB 3.3 MB/s eta 0:00:31\n",
      "   ---------------------------------------- 0.9/99.8 MB 6.4 MB/s eta 0:00:16\n",
      "   ---------------------------------------- 1.2/99.8 MB 6.3 MB/s eta 0:00:16\n",
      "    --------------------------------------- 1.9/99.8 MB 8.1 MB/s eta 0:00:13\n",
      "    --------------------------------------- 2.2/99.8 MB 8.3 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 2.9/99.8 MB 8.8 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 3.3/99.8 MB 9.5 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 4.3/99.8 MB 10.3 MB/s eta 0:00:10\n",
      "   - -------------------------------------- 4.9/99.8 MB 10.5 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 5.5/99.8 MB 10.7 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 6.1/99.8 MB 10.9 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 6.7/99.8 MB 11.0 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 7.4/99.8 MB 10.9 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 7.9/99.8 MB 11.0 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 8.5/99.8 MB 11.1 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 9.1/99.8 MB 11.2 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 9.7/99.8 MB 11.3 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 10.3/99.8 MB 12.4 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 10.9/99.8 MB 12.4 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 11.6/99.8 MB 12.8 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 12.1/99.8 MB 12.6 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 12.7/99.8 MB 13.4 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 13.3/99.8 MB 12.8 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 13.9/99.8 MB 12.8 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 14.5/99.8 MB 12.6 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 15.2/99.8 MB 12.6 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 15.7/99.8 MB 12.6 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 16.3/99.8 MB 12.6 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 16.9/99.8 MB 12.6 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 17.5/99.8 MB 12.6 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 18.1/99.8 MB 12.6 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 18.7/99.8 MB 12.6 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 19.2/99.8 MB 12.8 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 19.9/99.8 MB 12.6 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 20.5/99.8 MB 12.6 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 21.1/99.8 MB 12.6 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 21.7/99.8 MB 12.6 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 22.3/99.8 MB 12.6 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 22.9/99.8 MB 12.6 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 23.5/99.8 MB 12.6 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 24.1/99.8 MB 12.6 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 24.6/99.8 MB 12.6 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 25.3/99.8 MB 12.6 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 25.8/99.8 MB 12.6 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 26.5/99.8 MB 12.8 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 27.1/99.8 MB 12.6 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 27.7/99.8 MB 12.8 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 28.3/99.8 MB 12.8 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 28.9/99.8 MB 12.8 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 29.5/99.8 MB 12.8 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 30.1/99.8 MB 12.9 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 30.5/99.8 MB 12.8 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 31.3/99.8 MB 12.8 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 31.9/99.8 MB 12.8 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 32.5/99.8 MB 12.8 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 33.1/99.8 MB 12.8 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 33.7/99.8 MB 12.8 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 34.3/99.8 MB 12.8 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 34.9/99.8 MB 12.9 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 35.5/99.8 MB 12.8 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 36.0/99.8 MB 12.8 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 36.6/99.8 MB 12.9 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 37.2/99.8 MB 12.8 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 37.8/99.8 MB 12.8 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 38.4/99.8 MB 12.6 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 39.0/99.8 MB 12.6 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 39.6/99.8 MB 12.9 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 40.2/99.8 MB 12.6 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 40.7/99.8 MB 12.8 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 41.3/99.8 MB 12.6 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 41.9/99.8 MB 12.6 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 42.5/99.8 MB 12.6 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 43.1/99.8 MB 12.6 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 43.7/99.8 MB 12.6 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 44.1/99.8 MB 12.4 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 44.9/99.8 MB 12.6 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 45.5/99.8 MB 12.6 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 46.1/99.8 MB 12.6 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 46.7/99.8 MB 12.6 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 47.2/99.8 MB 12.6 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 47.8/99.8 MB 12.6 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 48.4/99.8 MB 12.6 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 49.0/99.8 MB 12.6 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 49.6/99.8 MB 12.6 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 50.2/99.8 MB 12.6 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 50.8/99.8 MB 12.6 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 51.4/99.8 MB 12.6 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 52.0/99.8 MB 12.6 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 52.6/99.8 MB 12.6 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 53.2/99.8 MB 12.6 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 53.8/99.8 MB 12.6 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 54.2/99.8 MB 12.6 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 55.0/99.8 MB 12.6 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 55.6/99.8 MB 12.6 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 56.2/99.8 MB 12.6 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 56.8/99.8 MB 12.6 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 57.4/99.8 MB 12.6 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 58.0/99.8 MB 12.6 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 58.6/99.8 MB 12.6 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 59.2/99.8 MB 12.6 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 59.8/99.8 MB 12.6 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 60.3/99.8 MB 12.8 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 60.9/99.8 MB 12.6 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 61.5/99.8 MB 12.6 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 62.1/99.8 MB 12.6 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 62.7/99.8 MB 12.8 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 63.3/99.8 MB 12.6 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 63.9/99.8 MB 12.6 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 64.5/99.8 MB 12.8 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 65.1/99.8 MB 12.6 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 65.6/99.8 MB 12.8 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 66.0/99.8 MB 13.1 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 66.8/99.8 MB 12.6 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 67.4/99.8 MB 12.6 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 67.9/99.8 MB 12.4 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 68.5/99.8 MB 12.4 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 69.0/99.8 MB 12.4 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 69.7/99.8 MB 12.4 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 70.3/99.8 MB 12.4 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 70.8/99.8 MB 12.6 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 71.5/99.8 MB 12.4 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 72.0/99.8 MB 12.6 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 72.7/99.8 MB 12.4 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 73.2/99.8 MB 12.4 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 73.9/99.8 MB 12.4 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 74.5/99.8 MB 12.4 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 75.0/99.8 MB 12.4 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 75.6/99.8 MB 12.4 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 76.2/99.8 MB 12.4 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 76.7/99.8 MB 12.3 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 77.3/99.8 MB 12.6 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 77.9/99.8 MB 12.4 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 78.5/99.8 MB 12.6 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 78.9/99.8 MB 12.6 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 79.3/99.8 MB 11.9 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 79.7/99.8 MB 11.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 79.8/99.8 MB 11.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 80.0/99.8 MB 10.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 80.1/99.8 MB 10.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 80.1/99.8 MB 10.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 80.3/99.8 MB 9.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 80.4/99.8 MB 9.5 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 80.5/99.8 MB 9.1 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 80.6/99.8 MB 8.7 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 80.7/99.8 MB 8.4 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 80.7/99.8 MB 8.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 80.8/99.8 MB 8.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 80.8/99.8 MB 7.6 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 80.9/99.8 MB 7.4 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 80.9/99.8 MB 7.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 81.0/99.8 MB 7.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 81.0/99.8 MB 6.7 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 81.1/99.8 MB 6.6 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 81.1/99.8 MB 6.5 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 81.2/99.8 MB 6.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 81.4/99.8 MB 6.1 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 81.5/99.8 MB 6.0 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 81.7/99.8 MB 5.8 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 81.8/99.8 MB 5.7 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 82.0/99.8 MB 5.6 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 82.1/99.8 MB 5.5 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 82.3/99.8 MB 5.5 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 82.4/99.8 MB 5.3 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 82.6/99.8 MB 5.2 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 82.9/99.8 MB 5.2 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 83.1/99.8 MB 5.1 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 83.3/99.8 MB 5.0 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 83.5/99.8 MB 5.0 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 83.7/99.8 MB 4.9 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 83.9/99.8 MB 4.8 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 84.1/99.8 MB 4.7 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 84.4/99.8 MB 4.7 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 84.7/99.8 MB 4.6 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 85.0/99.8 MB 4.6 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 85.3/99.8 MB 4.5 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 85.6/99.8 MB 4.5 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 86.0/99.8 MB 4.5 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 86.3/99.8 MB 4.4 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 86.6/99.8 MB 4.4 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 86.9/99.8 MB 4.4 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 87.3/99.8 MB 4.3 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 87.8/99.8 MB 4.3 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 88.2/99.8 MB 4.3 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 88.6/99.8 MB 4.3 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 89.1/99.8 MB 4.3 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 89.5/99.8 MB 4.3 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 90.0/99.8 MB 4.3 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 90.4/99.8 MB 4.6 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 91.0/99.8 MB 5.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 91.3/99.8 MB 5.8 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 91.8/99.8 MB 6.5 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 92.3/99.8 MB 6.9 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 92.8/99.8 MB 7.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 93.3/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 93.7/99.8 MB 8.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 94.2/99.8 MB 8.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 94.8/99.8 MB 9.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 95.4/99.8 MB 9.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 95.9/99.8 MB 9.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 96.5/99.8 MB 9.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 96.9/99.8 MB 10.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  97.4/99.8 MB 10.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  97.9/99.8 MB 10.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  98.3/99.8 MB 10.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  98.7/99.8 MB 10.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.3/99.8 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 99.8/99.8 MB 5.2 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.0.2\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "692fe96e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid classes inferred from unique values of `y`.  Expected: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47], got [1962 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989\n 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003\n 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017\n 2018 2019 2020 2021 2022 2023]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 16\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Assuming you have X_train, y_train, X_test, y_test as your prepared data\u001b[39;00m\n\u001b[0;32m      8\u001b[0m xgb_model \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier(  \u001b[38;5;66;03m# Instantiate the XGBoost model\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# Set the maximum depth of each tree to 3\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,  \u001b[38;5;66;03m# Set the learning rate to 0.1\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m11\u001b[39m  \u001b[38;5;66;03m# Set a random seed for reproducibility\u001b[39;00m\n\u001b[0;32m     14\u001b[0m )\n\u001b[1;32m---> 16\u001b[0m xgb_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)  \u001b[38;5;66;03m# Train the XGBoost model using the training data\u001b[39;00m\n\u001b[0;32m     18\u001b[0m y_pred_xgb \u001b[38;5;241m=\u001b[39m xgb_model\u001b[38;5;241m.\u001b[39mpredict(X_val)  \u001b[38;5;66;03m# Make predictions on the test data using the trained model\u001b[39;00m\n\u001b[0;32m     20\u001b[0m mae_xgb \u001b[38;5;241m=\u001b[39m mean_absolute_error(y_val, y_pred_xgb)\n",
      "File \u001b[1;32mD:\\06.DOWNLOAD\\anaconda3_2\\Lib\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\06.DOWNLOAD\\anaconda3_2\\Lib\\site-packages\\xgboost\\sklearn.py:1467\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1462\u001b[0m     expected_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m   1463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1464\u001b[0m     classes\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m expected_classes\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1465\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (classes \u001b[38;5;241m==\u001b[39m expected_classes)\u001b[38;5;241m.\u001b[39mall()\n\u001b[0;32m   1466\u001b[0m ):\n\u001b[1;32m-> 1467\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1468\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid classes inferred from unique values of `y`.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1469\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclasses\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1470\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_xgb_params()\n\u001b[0;32m   1474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid classes inferred from unique values of `y`.  Expected: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47], got [1962 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989\n 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003\n 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017\n 2018 2019 2020 2021 2022 2023]"
     ]
    }
   ],
   "source": [
    "#pip install xgboost\n",
    "#conda install -c conda-forge xgboost\n",
    "\n",
    "import xgboost as xgb  # Import the XGBoost library\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(  # Instantiate the XGBoost model\n",
    "    max_depth=3,  # Set the maximum depth of each tree to 3\n",
    "    learning_rate=0.1,  # Set the learning rate to 0.1\n",
    "    n_estimators=100,  # Use 100 trees in the ensemble\n",
    "    objective='binary:logistic',  # Use the logistic regression objective for binary classification\n",
    "    random_state=11  # Set a random seed for reproducibility\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)  # Train the XGBoost model using the training data\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(X_val)  # Make predictions on the test data using the trained model\n",
    "\n",
    "mae_xgb = mean_absolute_error(y_val, y_pred_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3985fd67",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4356c5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.494887348353553"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm  # Import the SVM module from scikit-learn\n",
    "\n",
    "# Step 3: Instantiate the SVM model\n",
    "svm_model = svm.SVC(kernel='linear', C=1.0, random_state=11)\n",
    "# Create an instance of the SVM model with a linear kernel, regularization parameter C of 1.0, and random state for \n",
    "# reproducibility\n",
    "\n",
    "# Step 4: Train the model\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Make predictions\n",
    "y_pred_svm = svm_model.predict(X_val)\n",
    "# Use the trained SVM model to predict the labels for the validation data\n",
    "\n",
    "mae_svm = mean_absolute_error(y_val, y_pred_svm)\n",
    "mae_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a44199f",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6c2cec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\06.DOWNLOAD\\anaconda3_2\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\06.DOWNLOAD\\anaconda3_2\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\06.DOWNLOAD\\anaconda3_2\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'num_classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m      6\u001b[0m num_channels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Step 3: Define the CNN architecture\u001b[39;00m\n\u001b[0;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[0;32m     10\u001b[0m     layers\u001b[38;5;241m.\u001b[39mConv2D(\u001b[38;5;241m32\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(image_width, image_height, num_channels)),\n\u001b[0;32m     11\u001b[0m     layers\u001b[38;5;241m.\u001b[39mMaxPooling2D(pool_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)),\n\u001b[0;32m     12\u001b[0m     layers\u001b[38;5;241m.\u001b[39mFlatten(),\n\u001b[0;32m     13\u001b[0m     layers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m64\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m---> 14\u001b[0m     layers\u001b[38;5;241m.\u001b[39mDense(num_classes, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m ])\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Create a sequential model with convolutional, pooling, flattening, and fully connected layers\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Step 4: Compile the model\u001b[39;00m\n\u001b[0;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_classes' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "image_width = 64\n",
    "image_height = 64\n",
    "num_channels = 3\n",
    "\n",
    "# Step 3: Define the CNN architecture\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(image_width, image_height, num_channels)),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "# Create a sequential model with convolutional, pooling, flattening, and fully connected layers\n",
    "\n",
    "# Step 4: Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# Configure the model with an optimizer, loss function, and additional metrics\n",
    "\n",
    "# Step 5: Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
    "# Fit the model to the training data (X_train, y_train) for a specified number of epochs and batch size\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "y_pred_cnn = model.predict(X_val)\n",
    "# Use the trained SVM model to predict the labels for the validation data\n",
    "\n",
    "mae_cnn = mean_absolute_error(y_val, y_pred_cnn)\n",
    "mae_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc1b326",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "421ac5a0",
    "e5GL8ZMmUIRQ",
    "620ecca3",
    "Cg-TXN_xLL6F",
    "JEWkvSp48l36",
    "kLwqsenRUg5H",
    "K3Tm8xJEh-8h",
    "-j0kgW6niiRA",
    "2Z-tlouXWPv9",
    "Cq4Njnxorbr6",
    "afe827be"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
